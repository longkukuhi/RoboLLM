{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ImageNet Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import ImageNetDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write ../datasets/imagenet/beit3/imagenet.train.index.jsonl with 1281167 items !\n",
      "Write ../datasets/imagenet/beit3/imagenet.val.index.jsonl with 50000 items !\n"
     ]
    }
   ],
   "source": [
    "ImageNetDataset.make_dataset_index(\n",
    "    train_data_path= \"../datasets/imagenet/imagenet/Data/CLS-LOC/train\",\n",
    "    val_data_path= \"../datasets/imagenet/imagenet/Data/CLS-LOC/val\",\n",
    "    index_path= \"../datasets/imagenet/beit3/\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:22.648972Z",
     "end_time": "2023-06-13T11:24:26.911954Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# COCO Captioning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from datasets import CaptioningDataset\n",
    "from transformers import XLMRobertaTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:46:01.696834Z",
     "end_time": "2023-06-13T11:46:01.707835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ../datasets/coco\\dataset_coco.json\n",
      "Find 113287 images and 566747 image-text pairs for karpathy dataset train split !\n",
      "Write ../datasets/coco\\coco_captioning.train.jsonl with 566747 items !\n",
      "read ../datasets/coco\\dataset_coco.json\n",
      "Find 5000 images and 5000 image-text pairs for karpathy dataset val split !\n",
      "Write ../datasets/coco\\coco_captioning.val.jsonl with 5000 items !\n",
      "read ../datasets/coco\\dataset_coco.json\n",
      "Find 5000 images and 5000 image-text pairs for karpathy dataset test split !\n",
      "Write ../datasets/coco\\coco_captioning.test.jsonl with 5000 items !\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLMRobertaTokenizer(\"beit3.spm\")\n",
    "\n",
    "CaptioningDataset.make_coco_captioning_dataset_index(\n",
    "    data_path=\"../datasets/coco\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:53:56.620824Z",
     "end_time": "2023-06-13T11:54:18.847055Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VQA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not all images have caption annotations\n",
      "82783 82774 82774\n",
      "Write ../datasets/coco\\vqa.train.jsonl with 434867 items !\n",
      "not all images have caption annotations\n",
      "40504 40503 40503\n",
      "Write ../datasets/coco\\vqa.val.jsonl with 210051 items !\n",
      "all images have caption annotations\n",
      "81434 81434 81434\n",
      "Write ../datasets/coco\\vqa.test.jsonl with 447793 items !\n",
      "not all images have caption annotations\n",
      "81434 36807 36807\n",
      "Write ../datasets/coco\\vqa.test-dev.jsonl with 107394 items !\n",
      "Contains 40503 image and 210051 pairs for val set!\n",
      "Write ../datasets/coco\\vqa.trainable_val.jsonl with 204726 items !\n",
      "Write ../datasets/coco\\vqa.rest_val.jsonl with 5325 items !\n"
     ]
    }
   ],
   "source": [
    "from datasets import VQAv2Dataset\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer(\"beit3.spm\")\n",
    "\n",
    "VQAv2Dataset.make_dataset_index(\n",
    "    data_path=\"../datasets/coco\",\n",
    "    tokenizer=tokenizer,\n",
    "    annotation_data_path=\"../datasets/coco/vqa\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T19:11:09.498065Z",
     "end_time": "2023-06-13T19:11:56.651280Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## retrieval dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ../datasets/coco\\dataset_coco.json\n",
      "Find 113287 images and 566747 image-text pairs for karpathy dataset train split !\n",
      "Write ../datasets/coco\\coco_retrieval.train.jsonl with 566747 items !\n",
      "read ../datasets/coco\\dataset_coco.json\n",
      "Find 5000 images and 25010 image-text pairs for karpathy dataset val split !\n",
      "Write ../datasets/coco\\coco_retrieval.val.jsonl with 25010 items !\n",
      "read ../datasets/coco\\dataset_coco.json\n",
      "Find 5000 images and 25010 image-text pairs for karpathy dataset test split !\n",
      "Write ../datasets/coco\\coco_retrieval.test.jsonl with 25010 items !\n"
     ]
    }
   ],
   "source": [
    "from datasets import RetrievalDataset\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer(\"beit3.spm\")\n",
    "\n",
    "RetrievalDataset.make_coco_dataset_index(\n",
    "    data_path=\"../datasets/coco\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T15:12:51.925839900Z",
     "start_time": "2023-07-03T15:12:24.795013700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# proprossing the movie dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "def get_sentencepiece_model_for_beit3():\n",
    "    from transformers import XLMRobertaTokenizer\n",
    "    return XLMRobertaTokenizer(\"./beit3.spm\")\n",
    "\n",
    "tokenizer = get_sentencepiece_model_for_beit3()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T20:29:06.516639Z",
     "end_time": "2023-06-15T20:29:07.109605Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import json\n",
    "ann = json.load(open(\"../zixuan_recsystem/data/Amazon_Sports_Dataset/raw_text.json\", \"r\"))\n",
    "items = []\n",
    "for image_id, data in ann.items():\n",
    "    tokens = tokenizer.tokenize(data)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    items.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"text_segment\": token_ids,\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T21:16:14.291653Z",
     "end_time": "2023-06-15T21:16:19.316048Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def _write_data_into_jsonl(items, jsonl_file):\n",
    "    with open(jsonl_file, mode=\"w\", encoding=\"utf-8\") as writer:\n",
    "        for data in items:\n",
    "            writer.write(json.dumps(data, indent=None))\n",
    "            writer.write('\\n')\n",
    "    print(\"Write %s with %d items !\" % (jsonl_file, len(items)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T20:35:35.690374Z",
     "end_time": "2023-06-15T20:35:35.712378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write ../zixuan_recsystem/data/Amazon_Sports_Dataset/raw_text_tokenize.json with 18357 items !\n"
     ]
    }
   ],
   "source": [
    "_write_data_into_jsonl(items, '../zixuan_recsystem/data/Amazon_Sports_Dataset/raw_text_tokenize.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T21:16:20.258187Z",
     "end_time": "2023-06-15T21:16:20.876084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset_names = ['Amazon_Elec_Dataset',\n",
    "                 'Amazon_Clothing_Dataset', 'Amazon_Baby_Dataset',\n",
    "                 ]\n",
    "#'Amazon_Sports_Dataset'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T20:39:18.513234Z",
     "end_time": "2023-06-15T20:39:18.544234Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Amazon_Elec_Dataset\n",
      "Write ../zixuan_recsystem/data/Amazon_Elec_Dataset/raw_text.json with 63001 items !\n",
      "Processing Amazon_Clothing_Dataset\n",
      "Write ../zixuan_recsystem/data/Amazon_Clothing_Dataset/raw_text.json with 23033 items !\n",
      "Processing Amazon_Baby_Dataset\n",
      "Write ../zixuan_recsystem/data/Amazon_Baby_Dataset/raw_text.json with 7050 items !\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "    ann = json.load(open(\"../zixuan_recsystem/data/%s/raw_text.json\" % dataset_name, \"r\"))\n",
    "    items = []\n",
    "    print(\"Processing %s\" % dataset_name)\n",
    "    for image_id, data in ann.items():\n",
    "        tokens = tokenizer.tokenize(data)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        items.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"tokens\": tokens,\n",
    "            \"text_segment\": token_ids,\n",
    "        })\n",
    "    _write_data_into_jsonl(items, \"../zixuan_recsystem/data/%s/raw_text.json\" % dataset_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T20:40:40.186335Z",
     "end_time": "2023-06-15T20:41:14.078731Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the raw text in AtoMiC dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def get_sentencepiece_model_for_beit3():\n",
    "    from transformers import XLMRobertaTokenizer\n",
    "    return XLMRobertaTokenizer(\"./beit3.spm\")\n",
    "\n",
    "tokenizer = get_sentencepiece_model_for_beit3()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:46:52.575776100Z",
     "start_time": "2023-07-16T09:46:51.901171600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def _write_data_into_jsonl(items, jsonl_file):\n",
    "    with open(jsonl_file, mode=\"w\", encoding=\"utf-8\") as writer:\n",
    "        for data in items:\n",
    "            writer.write(json.dumps(data, indent=None))\n",
    "            writer.write('\\n')\n",
    "    print(\"Write %s with %d items !\" % (jsonl_file, len(items)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:46:52.591778Z",
     "start_time": "2023-07-16T09:46:52.577778900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1431425/3002458 [11:18<12:41, 2062.19it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "dataset_path = \"../datasets/ATOMIC/\"\n",
    "splits = [\"train\",\"validation\"] #[\"train\", \"dev\", \"test\"]\n",
    "for split in splits:\n",
    "    items = []\n",
    "    ann = []\n",
    "    with open(f\"../datasets/ATOMIC/Atomic_text_{split}.json\", \"r\") as reader:\n",
    "        for line in reader:\n",
    "            data = json.loads(line)\n",
    "            ann.append(data)\n",
    "\n",
    "    for idx in tqdm(range(len(ann))):\n",
    "        new_dict = ann[idx]\n",
    "        # context_page_des_tokens = tokenizer.tokenize(new_dict[\"context_page_description\"])\n",
    "        context_sec_des_tokens = tokenizer.tokenize(new_dict[\"context_section_description\"])\n",
    "        page_title_tokens = tokenizer.tokenize(new_dict[\"page_title\"])\n",
    "        section_title_tokens = tokenizer.tokenize(new_dict[\"section_title\"])\n",
    "        category_tokens = tokenizer.tokenize(''.join(new_dict[\"category\"]))\n",
    "\n",
    "\n",
    "        # context_page_des_tokens_ids = tokenizer.convert_tokens_to_ids(context_page_des_tokens)\n",
    "        context_sec_des_tokens_ids = tokenizer.convert_tokens_to_ids(context_sec_des_tokens)\n",
    "        page_title_tokens_ids = tokenizer.convert_tokens_to_ids(page_title_tokens)\n",
    "        section_title_tokens_ids = tokenizer.convert_tokens_to_ids(section_title_tokens)\n",
    "\n",
    "        # new_dict[\"context_page_des_tokens\"] = context_page_des_tokens\n",
    "        # new_dict[\"context_sec_des_tokens\"] = context_sec_des_tokens\n",
    "        # new_dict[\"page_title_tokens\"] = page_title_tokens\n",
    "        # new_dict[\"section_title_tokens\"] = section_title_tokens\n",
    "        # new_dict[\"category_tokens\"] = category_tokens\n",
    "        # new_dict[\"context_page_des_tokens_ids\"] = context_page_des_tokens_ids\n",
    "        new_dict[\"context_sec_des_tokens_ids\"] = context_sec_des_tokens_ids\n",
    "        new_dict[\"page_title_tokens_ids\"] = page_title_tokens_ids\n",
    "        new_dict[\"section_title_tokens_ids\"] = section_title_tokens_ids\n",
    "\n",
    "        items.append(new_dict)\n",
    "    _write_data_into_jsonl(items, f\"../datasets/ATOMIC/Atomic_text_tokenized_{split}.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T10:08:24.858802600Z",
     "start_time": "2023-07-16T09:47:35.387402800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write images data into numpy file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/longk/.cache/huggingface/datasets/TREC-AToMiC___parquet/TREC-AToMiC--AToMiC-Images-v0.2-275960c34975be87/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Found cached dataset parquet (C:/Users/longk/.cache/huggingface/datasets/TREC-AToMiC___parquet/TREC-AToMiC--AToMiC-Qrels-v0.2-f1634624281fffa7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "split = 'train'\n",
    "images = load_dataset(\"TREC-AToMiC/AToMiC-Images-v0.2\", split=split)\n",
    "qrels = load_dataset(\"TREC-AToMiC/AToMiC-Qrels-v0.2\", split=split)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T22:25:30.502857500Z",
     "start_time": "2023-07-04T22:24:51.626784100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11019202/11019202 [00:03<00:00, 3187237.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_ids = set(qrels[\"image_id\"])\n",
    "id2pos = {}\n",
    "for pos, _id in tqdm(enumerate(images['image_id']), total=len(images['image_id'])):\n",
    "    id2pos[_id] = pos\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T22:25:48.967233600Z",
     "start_time": "2023-07-04T22:25:30.503858300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11019202/11019202 [00:03<00:00, 3285467.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train_image_ids = []\n",
    "for idx, image_id in tqdm(enumerate(images[\"image_id\"]), total=len(images[\"image_id\"])):\n",
    "    if image_id in image_ids:\n",
    "        train_image_ids.append(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T22:31:29.774406500Z",
     "start_time": "2023-07-04T22:31:15.775890400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "3386183"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T22:31:34.931147100Z",
     "start_time": "2023-07-04T22:31:34.911146900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_image_ids = []\n",
    "for image_id in tqdm(qrels[\"image_id\"], total=len(qrels[\"image_id\"])):\n",
    "    valid_image_ids.append({'image': images[id2pos[image_id]]['image']})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "110"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_image_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T21:40:13.078072700Z",
     "start_time": "2023-07-04T21:40:13.033562500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'image': <PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=443x256>}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_image_ids[0]['image']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T21:40:13.100072500Z",
     "start_time": "2023-07-04T21:40:13.048069400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "np.save(os.path.join(\"../datasets/ATOMIC\", f\"{split}_images.npy\"), valid_image_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T21:40:16.464528300Z",
     "start_time": "2023-07-04T21:40:15.846941100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_images = np.load(\"../datasets/ATOMIC/train_images.npy\", allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T21:40:50.101998900Z",
     "start_time": "2023-07-04T21:40:20.451018100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'image': <PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=443x256>}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T21:40:50.103998100Z",
     "start_time": "2023-07-04T21:40:50.102998Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# split the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "train_text_ids = set(dataset_qrels[\"train\"][\"text_id\"])\n",
    "validation_text_ids = set(dataset_qrels[\"validation\"][\"text_id\"])\n",
    "test_text_ids = set(dataset_qrels[\"test\"][\"text_id\"])\n",
    "\n",
    "train_image_ids = set(dataset_qrels[\"train\"][\"image_id\"])\n",
    "validation_image_ids = set(dataset_qrels[\"validation\"][\"image_id\"])\n",
    "test_image_ids = set(dataset_qrels[\"test\"][\"image_id\"])\n",
    "\n",
    "split_dict_texts = defaultdict(list)\n",
    "split_dict_images = defaultdict(list)\n",
    "\n",
    "for index, row in tqdm(\n",
    "        enumerate(dataset_text['train']['text_id']), total=len(dataset_text['train']), desc=\"build split dict\"\n",
    "):\n",
    "    if row in train_text_ids:\n",
    "        split_dict_texts[\"train\"].append(index)\n",
    "    elif row in validation_text_ids:\n",
    "        split_dict_texts[\"validation\"].append(index)\n",
    "    elif row in test_text_ids:\n",
    "        split_dict_texts[\"test\"].append(index)\n",
    "    else:\n",
    "        split_dict_texts[\"other\"].append(index)\n",
    "\n",
    "for index, row in tqdm(\n",
    "        enumerate(dataset_image['train']['image_id']), total=len(dataset_image['train']), desc=\"build split dict\"\n",
    "):\n",
    "    if row in train_image_ids:\n",
    "        split_dict_images[\"train\"].append(index)\n",
    "    elif row in validation_image_ids:\n",
    "        split_dict_images[\"validation\"].append(index)\n",
    "    elif row in test_image_ids:\n",
    "        split_dict_images[\"test\"].append(index)\n",
    "    else:\n",
    "        split_dict_images[\"other\"].append(index)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
